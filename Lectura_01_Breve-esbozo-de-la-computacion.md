### **Lectura 1: Breve Esbozo de la Computación**

#### 1. **Introducción**

La *informática*, *informatics* en inglés, es una de las disciplinas que más ha transformado el mundo moderno. Hoy en día, estamos tan inmersos en la tecnología que rara vez nos detenemos a pensar en los fundamentos de las herramientas que usamos a diario. Sin embargo, detrás de cada dispositivo y de cada programa, existen principios teóricos sólidos que han permitido su desarrollo, constituyen el corazón de la tecnología contemporánea y tienen aplicaciones en todos los campos imaginables: desde la salud y la educación hasta la administración de negocios y las finanzas. Estas disciplinas se denominan *ciencias de la computación* o *computer science*, en inglés.

Algunos piensan que las ciencias de la computación simplemente se tratan del "uso de computadoras". Esto es un error. Como dijo Denning en 1985: “las ciencias de la computación son el cuerpo de conocimiento que trata sobre el diseño, análisis, implementación y aplicación de procesos que transforman información” (Denning, 1985, p. 16). No se limitan a lo visible, a la computadora en sí, sino a los algoritmos, sistemas y datos que forman la base de la tecnología. Este campo de estudio ha logrado un impacto tan profundo que entenderlo hoy es casi una obligación.

Para ustedes, como estudiantes de administración, comprender los principios de la informática no solo es útil, sino esencial. Vivimos en un mundo digital. Esto significa que la capacidad para gestionar grandes cantidades de información, tomar decisiones rápidas e informadas y optimizar procesos, es clave para tener éxito en cualquier carrera. La informática son esos conocimientos que permiten transformar datos en decisiones, en cualquier campo.

Ahora, antes de seguir adelante, quiero aclarar un punto crucial: la diferencia entre *informática* y *ciencias de la computación*. Las ciencias de la computación se enfocan en la creación de las bases teóricas y técnicas de la computación; es la ciencia detrás de cómo funcionan las computadoras y los sistemas. En cambio, la *informática* es la aplicación de esos conocimientos a campos específicos, como la administración, la biomedicina o incluso el arte. Hoy exploraremos los orígenes de esta fascinante disciplina, desde las primeras calculadoras mecánicas hasta las modernas computadoras digitales, y veremos cómo las ciencias de la computación han moldeado el mundo que conocemos.

#### 2. **Antecedentes de la Computación**

Para entender dónde estamos, primero debemos entender de dónde venimos. La historia de la computación se remonta a miles de años atrás. ¿Sabían que uno de los primeros dispositivos de cálculo fue el ábaco, utilizado en la antigua Mesopotamia hace más de 4,000 años? Este sencillo dispositivo fue una revolución en su época, pero la humanidad no se detuvo ahí. ¿Sabían que los mayas, en México, inventaron el cero y un sistema numérico muy avanzado para su tiempo? Sin embargo, esos conocimientos ancestrales se perdieron con la conquista española.

En el siglo XIX, el matemático e inventor británico *Charles Babbage* concibió un dispositivo que marcaría un hito en la historia de la computación: la *máquina diferencial*. Diseñada para automatizar los cálculos matemáticos, esta máquina es considerada uno de los primeros precursores de las computadoras modernas. Pero Babbage no pudo completar su invento debido a las limitaciones tecnológicas de su tiempo.

Sin embargo, fue en *1936* cuando ocurrió el verdadero punto de inflexión en la historia de la computación. Ese año, un joven matemático llamado *Alan Turing* publicó un artículo titulado "On Computable Numbers", en el que propuso la idea de una máquina teórica capaz de realizar cualquier cálculo que fuera computable. Esta máquina, conocida como la *máquina de Turing*, podría resolver cualquier problema matemático que pudiera describirse mediante un conjunto finito de reglas. Turing afirmaba que "cualquier problema que pueda ser resuelto por un humano siguiendo un conjunto de instrucciones también puede ser resuelto por una máquina" (Turing, 1936). Este concepto sentó las bases de lo que hoy conocemos como computadoras digitales.

Turing no estaba solo en su trabajo. Otros pioneros, como *John von Neumann*, también estaban haciendo avances cruciales en la década de 1940. Von Neumann propuso en 1945 lo que hoy conocemos como la *arquitectura de von Neumann*, un modelo que sugería que las computadoras podían almacenar tanto los datos como los programas en la misma memoria. Esto permitió que las computadoras se volvieran mucho más flexibles y eficientes, algo que es fundamental para el funcionamiento de los dispositivos modernos (von Neumann, 1945).

#### 3. **La Computadora: Antes Humana hoy Electrónica**

Antes de la llegada de las computadoras electrónicas, los cálculos matemáticos más complejos eran realizados por personas, muchas de ellas mujeres, conocidas como *calculistas humanas* o *human computers*. Estas mujeres jugaban un papel crucial, especialmente durante la Segunda Guerra Mundial, cuando se necesitaban cálculos precisos para la balística y otros propósitos militares. Pero el proceso era increíblemente lento y propenso a errores.

Aquí es donde entra la *ENIAC (Electronic Numerical Integrator and Computer)*, la primera computadora electrónica de propósito general, construida en 1945. La ENIAC fue diseñada para calcular trayectorias balísticas para el ejército de Estados Unidos, pero su capacidad para realizar cálculos a velocidades sin precedentes la convirtió rápidamente en un hito de la ingeniería. Según Goldstine y Goldstine (1946), "con la introducción del ENIAC, los cálculos que antes tomaban meses podían ahora completarse en cuestión de minutos" (p. 8). Este avance no solo revolucionó el campo militar, sino que también marcó el inicio de una nueva era en la que las computadoras comenzarían a transformar todas las áreas del conocimiento.

La llegada de la ENIAC fue un momento decisivo en la historia. En 1947, dos años después de la creación de esta computadora, se fundó la *Association for Computing Machinery (ACM)*, la primera sociedad científica dedicada exclusivamente a la computación. Este fue el primer reconocimiento formal de que la computación era un campo propio del conocimiento. Y en *1958*, se reconocieron formalmente las *ciencias de la computación* como una disciplina autónoma (Denning, 1985, p. 16).

Es importante señalar que, con el surgimiento de la computación, también surgió un nuevo lenguaje. Conceptos como "*bit*" (abreviatura de *binary digit*), "*hardware*" y "*software*" se acuñaron en esta época y, desde entonces, han formado parte del vocabulario esencial para entender el funcionamiento de la tecnología. Hoy en día, los términos se han extendido a nuevos horizontes con el auge de la *inteligencia artificial*, los *big data*, y el *machine learning*.

#### 4. **Las Ciencias de la Computación**

Ahora que hemos visto los antecedentes y el contexto histórico, ¿qué son exactamente las *ciencias de la computación*? Como mencioné al principio, no se trata solo del uso de computadoras, sino del estudio de los fundamentos que hacen posible que estas máquinas funcionen. En palabras de Zavala (2011), "las ciencias de la computación nunca han sido una disciplina en el sentido tradicional, sino más bien un paraguas interdisciplinario que reúne métodos y teorías de diversas áreas" (p. 56). Este es uno de los mayores logros de esta disciplina: ha logrado penetrar en todas las áreas del conocimiento.

Dentro de sus áreas más importantes encontramos los *algoritmos*, que son secuencias de instrucciones paso a paso para resolver problemas específicos. Los algoritmos son el núcleo de todos los programas y aplicaciones que usamos. También están las *estructuras de datos*, que permiten organizar y almacenar información de manera eficiente. Sin estas estructuras, sería imposible manejar la inmensa cantidad de datos que procesamos a diario. Y, por supuesto, tenemos los *lenguajes de programación*, que son las herramientas que usamos para darle instrucciones a las computadoras. Todo esto está gestionado por los *sistemas operativos*, como Windows o Android, que son los encargados de coordinar todos los recursos del hardware.

Uno de los avances más importantes en la historia de las ciencias de la computación fue el desarrollo de *algoritmos* que permiten a las computadoras procesar datos de manera eficiente. Un algoritmo es, en esencia, una receta que, cuando se sigue correctamente, resuelve un problema de manera sistemática. El *17 de noviembre de 1951*, con el *Proyecto LEO* en Gran Bretaña, se ejecutó el primer programa de software del mundo para administrar un negocio (Zavala, 2011, p. 80). Este evento sentó las bases para la informática tal como la conocemos hoy, ya que permitió aplicar la computación a la administración empresarial.

Hoy en día, algoritmos más avanzados permiten el funcionamiento de *ERP* (Enterprise Resource Planning) y *CRM* (Customer Relationship Management), herramientas esenciales en la administración de negocios.

#### 5. **Conclusión**

Los fundamentos de la informática son mucho más que conceptos técnicos. Como hemos visto, son la base sobre la cual se construye gran parte de la tecnología que utilizamos en nuestra vida diaria. Desde los primeros cálculos manuales hasta las avanzadas computadoras electrónicas de hoy en día, la informática ha transformado nuestra forma de trabajar, pensar y tomar decisiones. Para ustedes, futuros administradores, el dominio de estos conceptos será crucial para optimizar procesos, gestionar datos con mayor eficiencia y tomar decisiones informadas.

El desafío de este curso es que, al finalizar, ustedes sean capaces no solo de entender cómo funcionan las herramientas digitales que ya utilizan, sino también de aplicarlas de manera creativa y estratégica para resolver problemas reales en el campo de la administración. Como dijo Denning (1985), “la computación es la disciplina que da forma al futuro, transformando la información en conocimiento y el conocimiento en acción” (p. 16). La informática no es solo una herramienta, es una puerta a un presente lleno de posibilidades. ¡Bienvenidos a este fascinante campo!

---

### Referencias

- Denning, P. J. (1985). What is Computer Science? *American Scientist*, *73*(1), 16-19. https://doi.org/10.1511/1985.1.16

- Goldstine, H.H., Goldstine, A. (1982). The Electronic Numerical Integrator and Computer (ENIAC). *In*: Randell, B. (eds). *The Origins of Digital Computers. Texts and Monographs in Computer Science* (pp. 359–373). Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-61812-3_29

- Turing, A. M. (1936). On Computable Numbers, with an Application to the Entscheidungsproblem. *Proceedings of the London Mathematical Society*, *s2-42*(1), 230–265. https://doi.org/10.1112/plms/s2-42.1.230

- von Neumann, J. (1945). *First Draft of a Report on the EDVAC*. Moore School of Electrical Engineering, University of Pennsylvania. http://eia.udg.edu/computadors/fitxers/EDVAC_sencer.pdf

- Zavala, J. (2011). *La ingeniería de software: Una discusión epistemológica*. Tesis de maestría, Universidad Autónoma Metropolitana, México. https://t.me/profeJesusZavala/18
